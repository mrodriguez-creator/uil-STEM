// UIL CS Written Test – Big-O Analysis
const BIG_O_PROBLEMS = [

  { id: "bo1", topic: "Big-O Analysis", difficulty: 1, question: "What is the time complexity of accessing an element in an array by index?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 0, hint: "Arrays provide direct access via memory offset", explanation: "Array access by index is O(1) — constant time via computed memory address." },
  { id: "bo2", topic: "Big-O Analysis", difficulty: 1, question: "What is the time complexity of a single <code>for</code> loop that iterates through n elements?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 2, hint: "One pass through n elements", explanation: "A single loop through n elements is O(n) — linear time." },
  { id: "bo3", topic: "Big-O Analysis", difficulty: 2, question: "What is the time complexity of two nested <code>for</code> loops, each iterating n times?", choices: ["O(n)", "O(n log n)", "O(n²)", "O(n³)", "O(2n)"], answer: 2, hint: "The inner loop runs n times for each of the n outer iterations", explanation: "n × n = n². Two nested loops each running n times is O(n²)." },
  { id: "bo4", topic: "Big-O Analysis", difficulty: 2, question: "What is the time complexity of binary search on a sorted array of n elements?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 1, hint: "Each step cuts the search space in half", explanation: "Binary search halves the search space each step: O(log n)." },
  { id: "bo5", topic: "Big-O Analysis", difficulty: 2, question: "What is the time complexity of linear search on an unsorted array?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 2, hint: "In the worst case, you check every element", explanation: "Linear search checks elements one by one: O(n) worst case." },
  { id: "bo6", topic: "Big-O Analysis", difficulty: 2, question: "What is the best-case time complexity of merge sort?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 3, hint: "Merge sort always divides and merges, regardless of input order", explanation: "Merge sort is O(n log n) in all cases (best, average, worst)." },
  { id: "bo7", topic: "Big-O Analysis", difficulty: 2, question: "What is the worst-case time complexity of quicksort?", choices: ["O(n)", "O(n log n)", "O(n²)", "O(n³)", "O(2ⁿ)"], answer: 2, hint: "This happens when the pivot is always the smallest or largest element", explanation: "Quicksort worst case is O(n²), occurring when the pivot consistently creates unbalanced partitions." },
  { id: "bo8", topic: "Big-O Analysis", difficulty: 3, question: "What is the average-case time complexity of quicksort?", choices: ["O(n)", "O(n log n)", "O(n²)", "O(n³)", "O(log n)"], answer: 1, hint: "On average, pivots create roughly balanced partitions", explanation: "Quicksort averages O(n log n) with random pivot selection." },
  { id: "bo9", topic: "Big-O Analysis", difficulty: 3, question: "What is the space complexity of merge sort?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 2, hint: "Merge sort needs auxiliary space for the merge step", explanation: "Merge sort requires O(n) extra space for the temporary arrays during merging." },
  { id: "bo10", topic: "Big-O Analysis", difficulty: 2, question: "Which of the following growth rates is the slowest (best performance) for large n?", choices: ["O(n²)", "O(n log n)", "O(n)", "O(log n)", "O(1)"], answer: 4, hint: "Constant time doesn't grow at all", explanation: "O(1) < O(log n) < O(n) < O(n log n) < O(n²). O(1) is the slowest growing (best)." },
  { id: "bo11", topic: "Big-O Analysis", difficulty: 3, question: "What is the time complexity of inserting an element at the beginning of an <code>ArrayList</code> with n elements?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 2, hint: "All existing elements must shift right by one position", explanation: "Inserting at index 0 requires shifting all n elements: O(n)." },
  { id: "bo12", topic: "Big-O Analysis", difficulty: 3, question: "What is the amortized time complexity of <code>ArrayList.add(element)</code> (appending to the end)?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 0, hint: "Though occasional resizing is O(n), it happens rarely enough", explanation: "Appending is O(1) amortized. Occasional array resizing costs O(n) but is infrequent enough to average out." },
  { id: "bo13", topic: "Big-O Analysis", difficulty: 3, question: "What is the average time complexity of <code>HashMap.get(key)</code>?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 0, hint: "Hash-based lookup is constant time on average", explanation: "HashMap.get() is O(1) average case using hash-based bucket lookup." },
  { id: "bo14", topic: "Big-O Analysis", difficulty: 3, question: "What is the time complexity of <code>TreeSet.add(element)</code>?", choices: ["O(1)", "O(log n)", "O(n)", "O(n log n)", "O(n²)"], answer: 1, hint: "TreeSet is backed by a balanced binary search tree", explanation: "TreeSet uses a Red-Black tree, so add() is O(log n)." },
  { id: "bo15", topic: "Big-O Analysis", difficulty: 2, question: "Simplify: O(3n² + 5n + 100)", choices: ["O(3n²)", "O(n²)", "O(n² + n)", "O(n)", "O(100)"], answer: 1, hint: "Big-O drops constants and lower-order terms", explanation: "Drop constants and lower-order terms: O(3n² + 5n + 100) = O(n²)." },
];
